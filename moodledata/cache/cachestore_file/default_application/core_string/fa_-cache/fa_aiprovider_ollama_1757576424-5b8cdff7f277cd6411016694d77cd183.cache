a:41:{s:25:"action:explain_text:model";s:22:"Text explanation model";s:30:"action:explain_text:model_help";s:44:"The model used to explain the provided text.";s:37:"action:explain_text:systeminstruction";s:18:"System instruction";s:42:"action:explain_text:systeminstruction_help";s:142:"This instruction is sent to the AI model along with the user's prompt. Editing this instruction is not recommended unless absolutely required.";s:26:"action:generate_text:model";s:21:"Text generation model";s:31:"action:generate_text:model_help";s:45:"The model used to generate the text response.";s:38:"action:generate_text:systeminstruction";s:18:"System instruction";s:43:"action:generate_text:systeminstruction_help";s:142:"This instruction is sent to the AI model along with the user's prompt. Editing this instruction is not recommended unless absolutely required.";s:27:"action:summarise_text:model";s:24:"Text summarisation model";s:32:"action:summarise_text:model_help";s:46:"The model used to summarise the provided text.";s:39:"action:summarise_text:systeminstruction";s:18:"System instruction";s:44:"action:summarise_text:systeminstruction_help";s:142:"This instruction is sent to the AI model along with the user's prompt. Editing this instruction is not recommended unless absolutely required.";s:17:"custom_model_name";s:17:"Custom model name";s:15:"enablebasicauth";s:27:"Enable basic authentication";s:20:"enablebasicauth_help";s:56:"Enable basic authentication for the Ollama API provider.";s:8:"endpoint";s:12:"API endpoint";s:13:"endpoint_help";s:43:"The API endpoint for the Ollama API server.";s:11:"extraparams";s:16:"Extra parameters";s:16:"extraparams_help";s:140:"Extra parameters can be configured here. We support JSON format. For example:
<pre>
{
    "temperature": 0.5,
    "max_tokens": 100
}
</pre>";s:11:"invalidjson";s:19:"Invalid JSON string";s:8:"password";s:8:"Password";s:13:"password_help";s:43:"The password used for basic authentication.";s:10:"pluginname";s:19:"Ollama API provider";s:16:"privacy:metadata";s:64:"The Ollama API provider plugin does not store any personal data.";s:50:"privacy:metadata:aiprovider_ollama:externalpurpose";s:246:"This information is sent to the Ollama API in order for a response to be generated. Your Ollama account settings may change how Ollama stores and retains this data. No user data is explicitly sent to Ollama or stored in Moodle LMS by this plugin.";s:40:"privacy:metadata:aiprovider_ollama:model";s:40:"The model used to generate the response.";s:45:"privacy:metadata:aiprovider_ollama:prompttext";s:59:"The user entered text prompt used to generate the response.";s:8:"settings";s:8:"Settings";s:13:"settings_help";s:71:"Adjust the settings below to customise how requests are sent to Ollama.";s:17:"settings_mirostat";s:8:"Mirostat";s:22:"settings_mirostat_help";s:131:"Mirostat is a neural text decoding algorithm for controlling perplexity. 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0. (Default: 0)";s:13:"settings_seed";s:4:"seed";s:18:"settings_seed_help";s:161:"Sets the random number seed to use for generation. Setting this to a specific number will make the model generate the same text for the same prompt. (Default: 0)";s:20:"settings_temperature";s:11:"temperature";s:25:"settings_temperature_help";s:176:"Temperature influences whether the output is more random and creative or more predictable. Increasing the temperature will make the model answer more creatively. (Default: 0.8)";s:14:"settings_top_k";s:5:"top_k";s:19:"settings_top_k_help";s:176:"Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)";s:14:"settings_top_p";s:5:"top_p";s:19:"settings_top_p_help";s:181:"Works together with top-k. A higher value (e.g. 0.95) will lead to more diverse text, while a lower value (e.g. 0.5) will generate more focused and conservative text. (Default: 0.9)";s:8:"username";s:8:"Username";s:13:"username_help";s:43:"The username used for basic authentication.";}